# dpg_buffer.py
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from collections import deque


@dataclass
class Experience:
    """Estrutura para armazenar experi√™ncias"""
    state: np.ndarray
    action: np.ndarray
    reward: float
    next_state: np.ndarray
    done: bool
    info: Dict[str, Any]
    group: int
    sub_phase: int
    quality: float
    skills: Dict[str, float]  # Habilidades demonstradas


class SkillTransferMap:
    """Mapeamento de habilidades transfer√≠veis entre grupos"""
    
    def __init__(self):
        self.skill_transfer_rules = {
            # Funda√ß√£o ‚Üí Desenvolvimento
            (1, 2): {
                "transferable_skills": ["estabilidade", "controle_postural", "progresso_basico"],
                "skill_weights": {"estabilidade": 0.6, "controle_postural": 0.3, "progresso_basico": 0.1},
                "relevance_threshold": 0.7
            },
            # Desenvolvimento ‚Üí Dom√≠nio
            (2, 3): {
                "transferable_skills": ["coordena√ß√£o", "controle_velocidade", "efici√™ncia"],
                "skill_weights": {"coordena√ß√£o": 0.4, "controle_velocidade": 0.4, "efici√™ncia": 0.2},
                "relevance_threshold": 0.8
            },
            # Regress√µes
            (2, 1): {
                "transferable_skills": ["estabilidade", "controle_postural"],
                "skill_weights": {"estabilidade": 0.7, "controle_postural": 0.3},
                "relevance_threshold": 0.6
            },
            (3, 2): {
                "transferable_skills": ["coordena√ß√£o", "efici√™ncia"],
                "skill_weights": {"coordena√ß√£o": 0.6, "efici√™ncia": 0.4},
                "relevance_threshold": 0.7
            }
        }
    
    def get_transfer_rules(self, old_group: int, new_group: int) -> Dict:
        """Obt√©m regras de transfer√™ncia para transi√ß√£o"""
        return self.skill_transfer_rules.get((old_group, new_group), {
            "transferable_skills": [],
            "skill_weights": {},
            "relevance_threshold": 0.5
        })
    
    def calculate_skill_relevance(self, experience: Experience, target_group: int) -> float:
        """Calcula relev√¢ncia da experi√™ncia para o grupo alvo"""
        rules = self.get_transfer_rules(experience.group, target_group)
        
        if not rules["transferable_skills"]:
            return 0.0
        
        relevance = 0.0
        for skill, weight in rules["skill_weights"].items():
            skill_value = experience.skills.get(skill, 0.0)
            relevance += skill_value * weight
        
        return relevance


class SmartBufferManager:
    """
    ESPECIALISTA EM MEM√ìRIA com Preserva√ß√£o Inteligente
    """
    
    def __init__(self, logger, config, max_core_experiences=1000):
        self.logger = logger
        self.config = config
        self.max_core_experiences = max_core_experiences
        
        # Sistema de mem√≥ria hier√°rquico
        self.group_buffers = {}
        self.core_buffer = deque(maxlen=max_core_experiences)
        self.current_group_buffer = []
        
        # Sistema de preserva√ß√£o
        self.skill_map = SkillTransferMap()
        self.preservation_stats = {
            "total_transitions": 0,
            "experiences_preserved": 0,
            "preservation_rate": 0.0
        }
        
        # Estat√≠sticas
        self.experience_count = 0
        self.group_transitions = 0
    
    def store_experience(self, experience_data: Dict):
        """Armazena experi√™ncia com an√°lise de habilidades"""
        experience = self._create_enhanced_experience(experience_data)
        
        group = experience_data.get("group_level", 1)
        sub_phase = experience_data["phase_info"].get("sub_phase", 0)
        
        # Armazenar hierarquicamente
        self._store_hierarchical(experience, group, sub_phase)
        
        # Armazenar no core se for fundamental
        if self._is_fundamental_experience(experience):
            self.core_buffer.append(experience)
        
        self.experience_count += 1
    
    def _create_enhanced_experience(self, data: Dict) -> Experience:
        """Cria experi√™ncia com an√°lise de habilidades"""
        state = data["state"]
        action = data["action"]
        reward = data["reward"]
        phase_info = data["phase_info"]
        metrics = data["metrics"]
        
        quality = self._calculate_experience_quality(state, action, reward, metrics)
        skills = self._analyze_experience_skills(metrics, phase_info)
        
        return Experience(
            state=state,
            action=action,
            reward=reward,
            next_state=state,
            done=False,
            info=phase_info,
            group=data.get("group_level", 1),
            sub_phase=phase_info.get("sub_phase", 0),
            quality=quality,
            skills=skills
        )
    
    def _analyze_experience_skills(self, metrics: Dict, phase_info: Dict) -> Dict[str, float]:
        """Analisa habilidades demonstradas na experi√™ncia"""
        skills = {}
        
        # Habilidade de estabilidade
        roll = metrics.get("roll", 0)
        pitch = metrics.get("pitch", 0)
        skills["estabilidade"] = 1.0 - min(abs(roll) + abs(pitch), 1.0)
        
        # Habilidade de progresso
        distance = metrics.get("distance", 0)
        skills["progresso_basico"] = min(distance / 2.0, 1.0)
        
        # Habilidade de coordena√ß√£o
        left_contact = metrics.get("left_contact", False)
        right_contact = metrics.get("right_contact", False)
        skills["coordena√ß√£o"] = 1.0 if left_contact != right_contact else 0.3
        
        # Habilidade de efici√™ncia
        steps = metrics.get("steps", 1)
        skills["efici√™ncia"] = min(distance / max(steps, 1), 1.0)
        
        # Habilidade de controle de velocidade
        speed = metrics.get("speed", 0)
        target_speed = phase_info.get('target_speed', 1.0)
        speed_error = abs(speed - target_speed)
        skills["controle_velocidade"] = 1.0 - min(speed_error / target_speed, 1.0)
        
        # Habilidade de controle postural
        skills["controle_postural"] = 1.0 - min(abs(pitch) * 2.0, 1.0)
        
        return skills
    
    def transition_with_preservation(self, old_group: int, new_group: int, adaptive_config: Dict):
        """Transi√ß√£o inteligente com preserva√ß√£o de aprendizado"""
        self.group_transitions += 1
        
        # 1. Coletar experi√™ncias do grupo antigo
        old_experiences = self.group_buffers.get(old_group, [])
        
        # 2. Filtrar experi√™ncias relevantes
        relevant_experiences = self._filter_relevant_experiences(old_experiences, new_group)
        
        # 3. Combinar com experi√™ncias fundamentais
        preserved_experiences = relevant_experiences + list(self.core_buffer)
        
        # 4. Aplicar pol√≠tica de preserva√ß√£o
        preservation_policy = adaptive_config.get("learning_preservation", "medium")
        final_experiences = self._apply_preservation_policy(preserved_experiences, preservation_policy)
        
        # 5. Atualizar buffers
        self.group_buffers[new_group] = final_experiences
        self.current_group_buffer = final_experiences
        
        # Atualizar estat√≠sticas
        self.preservation_stats["total_transitions"] += 1
        self.preservation_stats["experiences_preserved"] += len(final_experiences)
        self.preservation_stats["preservation_rate"] = (
            self.preservation_stats["experiences_preserved"] / 
            (self.preservation_stats["total_transitions"] * 1000 + 1e-8)
        )
        
        self.logger.info(f"üîÑ Preserva√ß√£o: {old_group}‚Üí{new_group}, "
                        f"Experi√™ncias: {len(final_experiences)}")
    
    def _filter_relevant_experiences(self, experiences: List[Experience], new_group: int) -> List[Experience]:
        """Filtra experi√™ncias relevantes para o novo grupo"""
        relevant = []
        
        for exp in experiences:
            relevance = self.skill_map.calculate_skill_relevance(exp, new_group)
            rules = self.skill_map.get_transfer_rules(exp.group, new_group)
            
            if relevance >= rules["relevance_threshold"]:
                relevant.append(exp)
        
        # Ordenar por relev√¢ncia
        relevant.sort(key=lambda x: self.skill_map.calculate_skill_relevance(x, new_group), 
                     reverse=True)
        
        return relevant
    
    def _apply_preservation_policy(self, experiences: List[Experience], policy: str) -> List[Experience]:
        """Aplica pol√≠tica de preserva√ß√£o"""
        policy_limits = {
            "high": 800,    # Alta preserva√ß√£o
            "medium": 500,  # Preserva√ß√£o m√©dia
            "low": 300      # Baixa preserva√ß√£o
        }
        
        limit = policy_limits.get(policy, 500)
        return experiences[:limit]
    
    def _calculate_experience_quality(self, state, action, reward, metrics) -> float:
        """Calcula qualidade da experi√™ncia"""
        quality = 0.0
        
        # Fator de recompensa
        quality += min(abs(reward) * 0.2, 1.0)
        
        # Fator de progresso
        progress = metrics.get("distance", 0)
        if progress > 0:
            quality += min(progress * 2.0, 1.0)
        
        # Fator de estabilidade
        stability = 1.0 - min(metrics.get("roll", 0) + metrics.get("pitch", 0), 1.0)
        quality += stability * 0.3
        
        return min(quality, 1.0)
    
    def _is_fundamental_experience(self, experience: Experience) -> bool:
        """Verifica se experi√™ncia √© fundamental"""
        return (experience.quality > 0.7 and 
                experience.reward > 0.5 and
                experience.skills.get("estabilidade", 0) > 0.6)
    
    def _store_hierarchical(self, experience: Experience, group: int, sub_phase: int):
        """Armazena experi√™ncia na hierarquia"""
        if group not in self.group_buffers:
            self.group_buffers[group] = []
        self.group_buffers[group].append(experience)
        
        self.current_group_buffer.append(experience)
        
        # Limitar tamanho
        if len(self.current_group_buffer) > 2000:
            self.current_group_buffer = self.current_group_buffer[-1500:]
    
    def get_training_batch(self, batch_size=32):
        """Retorna batch para treinamento"""
        if not self.current_group_buffer:
            return None
        
        available = self.current_group_buffer + list(self.core_buffer)
        
        if len(available) < batch_size:
            batch_size = len(available)
        
        # Amostragem por qualidade
        qualities = [exp.quality for exp in available]
        probabilities = np.array(qualities) / sum(qualities)
        
        indices = np.random.choice(len(available), size=batch_size, p=probabilities, replace=False)
        return [available[i] for i in indices]
    
    def get_status(self):
        """Retorna status com estat√≠sticas de preserva√ß√£o"""
        return {
            "total_experiences": self.experience_count,
            "core_experiences": len(self.core_buffer),
            "current_group_experiences": len(self.current_group_buffer),
            "group_transitions": self.group_transitions,
            "preservation_stats": self.preservation_stats,
            "groups_with_buffer": list(self.group_buffers.keys())
        }
    
    def get_metrics(self) -> Dict:
        """Retorna m√©tricas para monitoramento"""
        if not self.current_group_buffer:
            return {
                "buffer_avg_quality": 0,
                "buffer_avg_reward": 0,
                "core_buffer_size": len(self.core_buffer),
                "current_buffer_size": len(self.current_group_buffer),
                "learning_convergence": 0,
                "memory_efficiency": 0,
            }

        avg_quality = np.mean([exp.quality for exp in self.current_group_buffer])
        avg_reward = np.mean([exp.reward for exp in self.current_group_buffer])

        return {
            "buffer_avg_quality": avg_quality,
            "buffer_avg_reward": avg_reward,
            "core_buffer_size": len(self.core_buffer),
            "current_buffer_size": len(self.current_group_buffer),
            "learning_convergence": 0.5,  # Placeholder
            "memory_efficiency": self.preservation_stats.get("preservation_rate", 0.0),
        }